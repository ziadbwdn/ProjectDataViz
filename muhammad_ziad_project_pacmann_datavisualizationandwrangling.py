# -*- coding: utf-8 -*-
"""Muhammad_Ziad_Project_Pacmann_DataVisualizationAndWrangling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QZEOKgTIh0bihRAqCmZa8th3inVmZUFU

#**Muhammad Ziad - Project Pacmann AI - Data Visualization and Wrangling Class**

**Importing module**
"""

pip install pandas-profiling[notebook]

pip show pandas-profiling

pip install ydata-profiling

import ydata_profiling as yp

#General modules
import numpy as np
import pandas as pd
from tqdm import *

#For plotting
import matplotlib.pyplot as plt
from matplotlib.widgets import Slider, Button
import seaborn as sns

#For dealing with time string types
import datetime

#For Exploratory Data Analysis (EDA)
from ydata_profiling import ProfileReport

# Load the Drive helper and mount
from google.colab import drive
drive.mount('/content/drive')

"""**Data loading**"""

df = pd.read_csv('/content/drive/My Drive/ProjectPacmann/AirQuality.csv', sep = ';', decimal = ',')
df.head()

"""Removing unnamed columns"""

df.drop(['Unnamed: 15','Unnamed: 16'], axis=1, inplace=True, errors = 'ignore')
df.head()

"""**Removing Null/Faulty Sensor Readings**"""

#Replacing bad sensor readings designated by an entry of -200 with NaN

df.replace(to_replace = -200, value = np.nan, inplace = True)
df.info()

"""As we can look above, Column 4 ('NMCH') has a lot of faulty readings relative to all the other sensors. Let
me determine what the percent composition is of nulls with respect to the total values. This will help me
determine whether to keep that column or drop it.
"""

percent_NaN = []
columns = df.columns
for col in columns:
    pNaN =  (df[col].isna().sum()/df.shape[0]) * 100 #sum NaN instances in each column. Divide by total rows
    percent_NaN.append(pNaN)
nan_percent_df = pd.DataFrame(percent_NaN,
                              index=columns,
                              columns=['%_NaN_in_Column']).sort_values('%_NaN_in_Column',ascending = False)
nan_percent_df

"""More than 90% of the entries in that columns are null. If I were to use the dropna method on the entire
dataset **I'd end up losing 90% of our data**. We do still have a column with information about
Benzene (C6H6) concentrations through the columns C6H6(GT) and
PT08.S2 (NMHCH), so we haven't completely lost the entirety of the hydrocarbon readings. Instead, I will opt for
dropping this feature from the dataset, dropping the NaN rows and press forward.
"""

df.drop('NMHC(GT)', axis=1, inplace=True, errors = 'ignore')
df = df.dropna()
df.head()

df.info()

"""**Formatting time features for analysis purposes**"""

df['DateTime'] =  (df.Date) + ' ' + (df.Time)
df.DateTime = df.DateTime.apply(lambda x: datetime.datetime.strptime(x, '%d/%m/%Y %H.%M.%S'))
df

df['Weekday'] = df['DateTime'].dt.day_name()
df['Month']   = df['DateTime'].dt.month_name()
df['Hour']    = df['DateTime'].dt.hour
df['Date']    = pd.to_datetime(df['Date'], format='%d/%m/%Y')
df.drop('Time', axis=1, inplace=True, errors = 'ignore')
df.head()

df = df[['Date','Month', 'Weekday','DateTime', 'Hour', 'CO(GT)','PT08.S1(CO)', 'C6H6(GT)', 'PT08.S2(NMHC)',
         'NOx(GT)', 'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)', 'T', 'RH', 'AH']]
df.head()

df.info()

"""**Checkpoint for DataFrame Profiling**

These are the data features that we have at our tools from this dataset:
Date (dd/mm/yyyy)

*   Time (hh:mm:ss)
*   True hourly averaged concentration CO in mg/m^3 (reference analyzer)
*   PT08.S1 (tin oxide) hourly averaged sensor response (nominally CO targeted)
*   True hourly averaged overall Non-metallic HydroCarbons concentration in mg/m^3 (reference
analyzer)
*   True hourly averaged Benzene concentration in microg/m^3 (reference analyzer)
*   PT08.S2 (titania) hourly averaged sensor response (nominally NMHC targeted)
*   PT08.S3 (tungsten oxide) hourly averaged sensor response (nominally NOx targeted)
*   True hourly averaged NO2 concentration in microg/m^3 (reference analyzer)
*   PT08.S4 (tungsten oxide) hourly averaged sensor response (nominally NO2 targeted)
*   PT08.S5 (indium oxide) hourly averaged sensor response (nominally O3 targeted)
*   Temperature in °C
*   Relative Humidity (&)
*   AH Absolute Humidity

**Use pandas profiling to quickly get a quick overview
of what variables would be interesting to explore**
"""

df.reset_index(drop=True, inplace=True) #Drop index prior to generating report
report = ProfileReport(df)
report

"""There's a few interesting things that report has shown.

In particular, it can be seen that all the pollutant concentrations detected by the sensors are highly
correlated with one another. Interestingly, for some reason the readings from the sensor 'PT08.S3(NOx)' are
anticorrelated with one another. This would suggest that the levels of Nitrous Oxide(s) are decreasing with
increasing levels of the other pollutants. Why would this be? Is this a real effect?

Other things that are noteworthy are the distribution shapes for each variable. Most of them appear to have
approximately unimodal distributions (that also show right-skewedness). Some of the exceptions are
PT08.S4 (NO2) are AH where there is fairly clear bimodal behavior occurring. This suggests that there is
further cleaning up that I need to do since skewedness in a distribution is related to the presence of outliers

**Outliers removing**
"""

# Removing Outliers with the Interquartile Range Method (IQR)

Q1 = df.quantile(0.25, numeric_only=True)  # First quartile (25%)
Q3 = df.quantile(0.75, numeric_only=True)  # Third quartile (75%)


IQR = Q3 - Q1 # IQR = InterQuartile Range

scale = 1.4 # May need to play with this value to modify outlier detection sensitivity if need be
lower_lim = Q1 - scale*IQR
upper_lim = Q3 + scale*IQR

cols = df.columns[5:]  # Look for outliers in columns starting from CO(GT)

# Align lower_lim and upper_lim with the DataFrame columns
lower_lim = lower_lim[cols]
upper_lim = upper_lim[cols]

# Mask a condition that removes rows with values above/below IQR limits
condition = ~((df[cols] < lower_lim) | (df[cols] > upper_lim)).any(axis=1)

# Generate a new DataFrame with outliers removed
df_filtered = df[condition]

df_filtered.info()

"""**Profile checking**

Now the data is free from the outliers

Let's move to another segment, which is dealing with skewedness of the data and how to treat them
"""

df_filtered.reset_index(drop=True, inplace=True) #Drop index prior to generating report
report = ProfileReport(df_filtered)
report

"""As the thing we have to recall, C6H6 is a hydrocarbon and we have another sensor that is detecting nonmetallic
hydrocarbons ģNMHCĤ. Therefore, the C6H6 column is sorta redundant and we can drop it without losing too
much information.

With NOx, we still have a sensor in the dataframe responsible for detecting nitrous oxides. This means that
the NOx column is similarly redundant.

With CO(GT), we still have a sensor in the dataframe responsible for specifically detecting CO. This means
that the CO(GT) column is also redundant.

Using that same logic, We will try drop the NO2 (GT) column as well.
As such, We will try toremove them from the dataset.

**Drop the unnecessary data**
"""

df_filtered.drop(['CO(GT)']  ,axis=1, inplace=True, errors = 'ignore')
df_filtered.drop(['NOx(GT)'] ,axis=1, inplace=True, errors = 'ignore')
df_filtered.drop(['C6H6(GT)'],axis=1, inplace=True, errors = 'ignore')
df_filtered.drop(['NO2(GT)'] ,axis=1, inplace=True, errors = 'ignore')
df_filtered.head()

"""**generate the final report before continuing to check the state of data itself**"""

df_filtered.reset_index(drop=True, inplace=True) #Drop index prior to generating report
report = ProfileReport(df_filtered)
report

"""**Time Series on Air Quality Data**"""

month_df_list = []
day_df_list   = []
hour_df_list  = []

months = ['January','February','March', 'April', 'May','June',
          'July', 'August', 'September', 'October', 'November', 'December']

days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']

for month in months:
    temp_df = df_filtered.loc[(df_filtered['Month'] == month)]
    month_df_list.append(temp_df)

for day in days:
    temp_df = df_filtered.loc[(df_filtered['Weekday'] == day)]
    day_df_list.append(temp_df)

for hour in range(24):
    temp_df = df_filtered.loc[(df_filtered['Hour'] == hour)]
    hour_df_list.append(temp_df)

def df_time_plotter(df_list, time_unit, y_col):

    months = ['January','February','March', 'April', 'May','June',
              'July', 'August', 'September', 'October', 'November', 'December']

    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']

    if time_unit == 'M':
        nRows = 3
        nCols = 4
        n_iter = len(months)
    elif time_unit == 'D':
        nRows = 2
        nCols = 4
        n_iter = len(days)
    elif time_unit == 'H':
        nRows = 4
        nCols = 6
        n_iter = 24
    else:
        print('time_unit must be a string equal to M,D, or H')
        return 0

    fig, axs = plt.subplots(nrows=nRows, ncols=nCols, figsize = (40,30))
    axs = axs.ravel()
    for i in range(n_iter):
        data = df_list[i]
        ax = axs[i]
        data.plot(kind ='scatter', x = 'DateTime', y= y_col , ax = ax, fontsize = 24)
        ax.set_ylabel('Pollutant Concentration',fontsize=30)
        ax.set_xlabel('')
        if time_unit == 'M':
            ax.set_title(y_col + ' ' + months[i],  size=40) # Title
        elif time_unit == 'D':
            ax.set_title(y_col + ' ' + days[i],  size=40) # Title
        else:
             ax.set_title(y_col + ' ' + str(i),  size=40) # Title
        ax.tick_params(labelrotation=60)

        #plt.xlim([datetime.date(2004, 3, 10), datetime.date(2004, 3, 30)])
    # set the spacing between subplots
    plt.subplots_adjust(left=0.1,
                    bottom=0.1,
                    right=0.9,
                    top=0.9,
                    wspace=0.4,
                    hspace=0.5)
    plt.show() # Depending on whether you use IPython or interactive mode, etc.

df_time_plotter(month_df_list,'M','PT08.S1(CO)')
df_time_plotter(month_df_list,'M','PT08.S2(NMHC)')

df_time_plotter(day_df_list,'D','PT08.S1(CO)')
df_time_plotter(day_df_list,'D','PT08.S2(NMHC)')

df_time_plotter(hour_df_list,'H','PT08.S1(CO)')
df_time_plotter(hour_df_list,'H','PT08.S2(NMHC)')

"""It looks like readings are quite low between 4-6 AM. The CO levels rise starting at 1PM and peak at around 6-8pm. Maybe a bar plot will make some of these relationships more apparent"""

sns.barplot(x = 'Month', y = 'PT08.S1(CO)', data = df_filtered)
plt.title('CO Values Per Month')
plt.xticks(rotation=90)
plt.show()

sns.barplot(x = 'Month', y = 'PT08.S2(NMHC)', data = df_filtered, color='orange')
plt.title('NMHC Values Per Month')
plt.xticks(rotation=90)
plt.show()

sns.barplot(x = 'Weekday', y = 'PT08.S1(CO)', data = df_filtered)
plt.title('CO Values Per Day of the Week')
plt.xticks(rotation=90)
plt.show()

sns.barplot(x = 'Weekday', y = 'PT08.S2(NMHC)', data = df_filtered, color ='orange')
plt.title('NMHC Values Per Day of the Week')
plt.xticks(rotation=90)
plt.show()

sns.barplot(x = 'Hour', y = 'PT08.S1(CO)', data = df_filtered)
plt.title('CO Values Per Hour')
plt.xticks(rotation=90)
plt.show()

sns.barplot(x = 'Hour', y = 'PT08.S2(NMHC)', data = df_filtered, color='orange')
plt.title('NMHC Values Per Hour')
plt.xticks(rotation=90)
plt.show()

df_final = df_filtered.iloc[:, 5:]
df_final

"""This is a lot clearer.

From these plots we can see the following:

*   October has the highest CO readings while August had the lowest readings.
*   October has the highest NMHC readings while February had the lowest readings, and 2nd lowest readings in August
*   CO levels trend downward from October to August. They start to rise between August to October.
*   NMHC levels trend downward from October to August. They start to rise between August to October.
*   CO and NMHC levels are lowest on Sundays and highest on Friday
*   CO levels are lowest between 4-5 AM, meanwhile NMHC levels are lowest between 3-5 AM and highest at 8 AM and 7 PM. This makes sense since these are rush hour times.

Let me generate a pairplot to get a better grasp of all these relationships.
"""

sns.set(font_scale=1.5)
sns.pairplot(df_final)

df_final.info()